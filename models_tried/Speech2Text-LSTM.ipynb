{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, Activation, Bidirectional, ConvLSTM2D, Attention, Dense, Flatten, MaxPool3D, MaxPool2D,BatchNormalization, Conv3D, GRU\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.backend import ctc_batch_cost, ctc_decode, ctc_label_dense_to_sparse, get_value\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import Levenshtein as Lev\n",
    "import sys\n",
    "from string import ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./LibriSpeech100/train/train_all/\"\n",
    "dev_path = \"./LibriSpeech100/dev/dev_all/\"\n",
    "test_path = \"./LibriSpeech100/test/test_all/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, path, to_fit = True):\n",
    "        self.path = path\n",
    "        self.list_X, self.list_Y = self.getLists()\n",
    "        self.to_fit = to_fit\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.list_X)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):      \n",
    "        dict_X = self.get_dict_X(index)   \n",
    "        dict_Y = self.get_dict_Y(index)\n",
    "        \n",
    "        X, Y, input_len, label_len, y_strings = self.generate_XY(dict_X, dict_Y)\n",
    "            \n",
    "        return [X, y_strings, input_len, label_len], Y\n",
    "    \n",
    "    \n",
    "    def getLists(self):\n",
    "        list_X = []\n",
    "        list_Y = []\n",
    "        for item in sorted(os.listdir(self.path)):\n",
    "            ext = item.split(\".\")[-1]\n",
    "            if ext == 'pkl':\n",
    "                list_X.append(item)\n",
    "            elif ext == 'txt':\n",
    "                list_Y.append(item)\n",
    "        return list_X, list_Y\n",
    "    \n",
    "    \n",
    "    def get_dict_X(self, index):\n",
    "        file_name = self.path + self.list_X[index]\n",
    "        with open(file_name, 'rb') as pickle_file:\n",
    "            dict_X = pickle.load(pickle_file)\n",
    "        return dict_X\n",
    "    \n",
    "    \n",
    "    def get_dict_Y(self, index):\n",
    "        filename = self.path + self.list_Y[index]\n",
    "        file = open(filename)\n",
    "        dict_Y = {}\n",
    "        for line in file:\n",
    "            data = line.split()\n",
    "            key = data[0]\n",
    "            value = ' '.join(data[1:])\n",
    "            dict_Y[key] = value\n",
    "        return dict_Y\n",
    "\n",
    "    \n",
    "    def generate_XY(self, dict_X, dict_Y):\n",
    "        X = []\n",
    "        Y = []\n",
    "        Y_strings = []\n",
    "        input_len = []\n",
    "        label_len = []\n",
    "        \n",
    "        max_x = 0\n",
    "        max_y = 0\n",
    "        \n",
    "        for key in dict_X:\n",
    "            x_temp = dict_X[key]\n",
    "            y_temp = dict_Y[key]\n",
    "            if max_x < x_temp.shape[1]:\n",
    "                max_x = x_temp.shape[1]\n",
    "            if max_y < len(y_temp):\n",
    "                max_y = len(y_temp)\n",
    "        \n",
    "        for key in dict_X:\n",
    "            x_temp = dict_X[key]\n",
    "            y_temp = dict_Y[key]\n",
    "            Y_strings.append(y_temp)\n",
    "\n",
    "            input_len.append(x_temp.shape[1])\n",
    "            label_len.append(len(y_temp))\n",
    "            \n",
    "            to_pad_x = ( (0,0), (0, max_x - dict_X[key].shape[1]))\n",
    "            to_pad_y = (  (0, max_y - len(dict_Y[key])))\n",
    "            \n",
    "            x_temp = np.pad(dict_X[key], pad_width = to_pad_x, mode='constant', constant_values=0)\n",
    "            y_temp = self.generate_Y_array(dict_Y[key], max_y)\n",
    "            X.append(x_temp.T)\n",
    "            Y.append(y_temp)\n",
    "          \n",
    "        return np.stack(X), np.stack(Y), np.stack(input_len), np.stack(label_len), Y_strings\n",
    "\n",
    "    \n",
    "    def generate_Y_array(self, sentence, maxlen):\n",
    "        space_token = ' '\n",
    "        end_token = '>'\n",
    "        blank_token = '%'\n",
    "        apos_token = '\\''\n",
    "        while len(sentence) != maxlen:\n",
    "            sentence += blank_token\n",
    "        sentence += end_token\n",
    "        \n",
    "        alphabet = list(ascii_uppercase) + [space_token, apos_token, blank_token, end_token] \n",
    "        char_to_index = {}\n",
    "        for idx, char in enumerate(alphabet):\n",
    "            char_to_index[char] = idx\n",
    "\n",
    "        y = []\n",
    "        \n",
    "        for char in sentence:\n",
    "            y.append(char_to_index[char])\n",
    "        \n",
    "        return np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataGenerator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataGenerator(train_path)\n",
    "val_data = DataGenerator(dev_path)\n",
    "test_data = DataGenerator(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data loaded by the DataGenerator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_data[0]\n",
    "x, y_strings, input_len, label_len = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 727, 20)\n",
      "(58,)\n",
      "(58,)\n",
      "58\n",
      "(58, 319)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(input_len.shape)\n",
    "print(label_len.shape)\n",
    "\n",
    "print(len(y_strings))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "int64\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(x.dtype)\n",
    "print(y.dtype)\n",
    "print(input_len.dtype)\n",
    "print(label_len.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND THAT IF SHE NOTICED ANYTHING ODD OR OUT OF PLACE SHE WOULD NEVER REST UNTIL SHE HAD FERRETED OUT THE WHYS AND WHEREFORES THEREOF THERE ARE PLENTY OF PEOPLE IN AVONLEA AND OUT OF IT WHO CAN ATTEND CLOSELY TO THEIR NEIGHBOR'S BUSINESS BY DINT OF NEGLECTING THEIR OWN\n"
     ]
    }
   ],
   "source": [
    "print(y_strings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(s1, s2):\n",
    "\n",
    "    s1 =s1.lower()\n",
    "    s2 =s2.lower()\n",
    "    b = set(s1.lower().split() + s2.lower().split())\n",
    "    \n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))/float(len(s2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a tensor array to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_string(indices):\n",
    "#     print(indices)\n",
    "    space_token = ' '\n",
    "    end_token = '>'\n",
    "    blank_token = '%'\n",
    "    apos_token = '\\''\n",
    "        \n",
    "    alphabet = list(ascii_uppercase) + [space_token, apos_token, blank_token, end_token] \n",
    "\n",
    "    sentence = ''\n",
    "    for idx in indices:\n",
    "        sentence += alphabet[idx]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(Model):\n",
    "    def __init__(self, op_dim = 30):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.rnn = LSTM(20, return_sequences= True)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.time_dense = TimeDistributed(Dense(op_dim))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.rnn(inputs)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.time_dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRModel(Model):\n",
    "    def __init__(self):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.base_model = BaseModel()\n",
    "        self.activation = Activation('softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.base_model(inputs)\n",
    "        x = self.activation(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASRModel()\n",
    "model.build(input_shape = (None, None, 20))\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, x, y_true, input_len, label_len, y_strings):\n",
    "#     print('------------------------------')\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     print(input_len.shape)\n",
    "#     print(label_len.shape)\n",
    "    \n",
    "    input_len = np.expand_dims(input_len, axis = 1)\n",
    "    label_len = np.expand_dims(label_len, axis = 1)\n",
    "#     print(input_len.shape)\n",
    "#     print(label_len.shape)\n",
    "            \n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "#         print(y_pred.shape)\n",
    "        loss = ctc_batch_cost(y_true, y_pred, input_len, label_len)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    input_len = np.squeeze(input_len)\n",
    "    y_decode = ctc_decode(y_pred, input_len)[0][0]\n",
    "    \n",
    "#         print(y_decode)\n",
    "#         print(len(y_strings))\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(len(y_strings)):\n",
    "        predicted_sentence = indices_to_string(y_decode[i])\n",
    "#             print(predicted_sentence)\n",
    "        accuracy += wer(predicted_sentence, y_strings[i])\n",
    "            \n",
    "    return tf.reduce_mean(loss), accuracy/len(y_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, x, y_true, input_len, label_len, y_strings, test = False):\n",
    "    input_len = np.expand_dims(input_len, axis = 1)\n",
    "    label_len = np.expand_dims(label_len, axis = 1)\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = ctc_batch_cost(y_true, y_pred, input_len, label_len)\n",
    "    \n",
    "    input_len = np.squeeze(input_len)\n",
    "    y_decode = ctc_decode(y_pred, input_len)[0][0]\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(len(y_strings)):\n",
    "        predicted_sentence = indices_to_string(y_decode[i])\n",
    "#             print(predicted_sentence)\n",
    "        accuracy += wer(predicted_sentence, y_strings[i])\n",
    "        \n",
    "        if test:\n",
    "            print(\"Correct Sentence:\", y_strings[i])\n",
    "            print(\"Predicted Sentence:\", predicted_sentence)\n",
    "    \n",
    "    return tf.reduce_mean(loss), accuracy/len(y_strings)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, val_ds, test = False):\n",
    "    val_step = 0\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "            \n",
    "    for inputs, y in val_ds:\n",
    "        x, y_strings, ip_len, label_len = inputs\n",
    "        val_step += 1       \n",
    "        loss, accuracy = validate(model, x, y, ip_len, label_len, y_strings, test)\n",
    "        val_loss += loss\n",
    "        val_accuracy += accuracy\n",
    "                \n",
    "    val_loss /= val_step\n",
    "    val_accuracy /= val_step\n",
    "\n",
    "    tf.print(' Validation Loss:', val_loss, ' Validation WER: ', val_accuracy)\n",
    "    \n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, optimizer, train_ds, val_ds = None,epochs=20):\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        step = 0\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "        for inputs, y in train_ds:\n",
    "            x, y_strings, ip_len, label_len = inputs\n",
    "            step += 1\n",
    "            loss, accuracy = train_one_step(model, optimizer, x, y, ip_len, label_len, y_strings)\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "            if step % 78 == 0:\n",
    "                print(step)\n",
    "                \n",
    "            \n",
    "        epoch_loss /= step\n",
    "        epoch_accuracy /= step\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        tf.print('Epoch: ', epoch+1, ' Loss:', epoch_loss, ' WER: ', epoch_accuracy)\n",
    "        \n",
    "        \n",
    "        if val_ds:\n",
    "            val_loss, val_accuracy = model_evaluate(model, val_ds)\n",
    "            val_losses.append(val_loss)\n",
    "            val_acc.append(val_accuracy)\n",
    "        \n",
    "                \n",
    "    if not val_ds:    \n",
    "        return losses, accuracies\n",
    "    \n",
    "    return losses, accuracies, val_losses, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rushabh/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5783: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  1  Loss: 801.387817  WER:  1.0001261901615912\n",
      " Validation Loss: 440.161407  Validation WER:  1.0052382288665427\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  2  Loss: 687.57843  WER:  1.0015563333238935\n",
      " Validation Loss: 322.744507  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  3  Loss: 529.411316  WER:  1.0\n",
      " Validation Loss: 320.116089  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  4  Loss: 526.972168  WER:  1.0\n",
      " Validation Loss: 318.176178  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  5  Loss: 522.903442  WER:  1.0\n",
      " Validation Loss: 314.986786  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  6  Loss: 515.773682  WER:  1.0\n",
      " Validation Loss: 310.454651  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  7  Loss: 509.274719  WER:  1.0\n",
      " Validation Loss: 308.051514  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  8  Loss: 505.46817  WER:  1.0\n",
      " Validation Loss: 306.542236  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  9  Loss: 502.679596  WER:  1.0\n",
      " Validation Loss: 305.26123  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  10  Loss: 500.680298  WER:  1.0\n",
      " Validation Loss: 304.527191  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  11  Loss: 498.998871  WER:  1.0\n",
      " Validation Loss: 303.695953  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  12  Loss: 497.236176  WER:  1.0\n",
      " Validation Loss: 302.892609  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  13  Loss: 495.61  WER:  1.0\n",
      " Validation Loss: 301.650482  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  14  Loss: 493.02655  WER:  0.9999992877492876\n",
      " Validation Loss: 300.341187  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  15  Loss: 491.368225  WER:  0.9999987723705672\n",
      " Validation Loss: 299.593597  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  16  Loss: 490.194702  WER:  0.9999946182354608\n",
      " Validation Loss: 299.061493  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  17  Loss: 489.235626  WER:  0.9999918911494003\n",
      " Validation Loss: 298.542206  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  18  Loss: 488.427399  WER:  0.9999910620227912\n",
      " Validation Loss: 298.133881  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  19  Loss: 487.73233  WER:  0.9999894166469259\n",
      " Validation Loss: 297.810638  Validation WER:  1.0\n",
      "78\n",
      "156\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[726,126,30] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Log]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1e1f379dfae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-9a747defd40a>\u001b[0m in \u001b[0;36mmodel_fit\u001b[0;34m(model, optimizer, train_ds, val_ds, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-c42c57e3bc59>\u001b[0m in \u001b[0;36mtrain_one_step\u001b[0;34m(model, optimizer, x, y_true, input_len, label_len, y_strings)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         print(y_pred.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctc_batch_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mctc_batch_cost\u001b[0;34m(y_true, y_pred, input_length, label_length)\u001b[0m\n\u001b[1;32m   5730\u001b[0m       ctc_label_dense_to_sparse(y_true, label_length), dtypes_module.int32)\n\u001b[1;32m   5731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5732\u001b[0;31m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5734\u001b[0m   return array_ops.expand_dims(\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   5714\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5715\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5716\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5717\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5718\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[726,126,30] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Log]"
     ]
    }
   ],
   "source": [
    "losses, accuracies, val_losses, val_acc = model_fit(model, optimizer, train_data, val_ds = val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, acc = model_evaluate(model, test_data, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89480caa3f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'acc' is not defined"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
