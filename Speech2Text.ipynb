{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TimeDistributed, Activation, Bidirectional, ConvLSTM2D, Attention, Dense, Flatten, MaxPool3D, MaxPool2D,BatchNormalization, Conv3D, GRU\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.backend import ctc_batch_cost, ctc_decode, ctc_label_dense_to_sparse, get_value\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import Levenshtein as Lev\n",
    "import sys\n",
    "from string import ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./LibriSpeech100/train/train_all/\"\n",
    "dev_path = \"./LibriSpeech100/dev/dev_all/\"\n",
    "test_path = \"./LibriSpeech100/test/test_all/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, path, to_fit = True):\n",
    "        self.path = path\n",
    "        self.list_X, self.list_Y = self.getLists()\n",
    "        self.to_fit = to_fit\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.list_X)\n",
    "    \n",
    "    def __getitem__(self, index):      \n",
    "        dict_X = self.get_dict_X(index)   \n",
    "        dict_Y = self.get_dict_Y(index)\n",
    "        \n",
    "        X, Y, input_len, label_len, y_strings = self.generate_XY(dict_X, dict_Y)\n",
    "            \n",
    "        return [X, y_strings, input_len, label_len], Y\n",
    "    \n",
    "    def getLists(self):\n",
    "        list_X = []\n",
    "        list_Y = []\n",
    "        for item in sorted(os.listdir(self.path)):\n",
    "            ext = item.split(\".\")[-1]\n",
    "            if ext == 'pkl':\n",
    "                list_X.append(item)\n",
    "            elif ext == 'txt':\n",
    "                list_Y.append(item)\n",
    "        return list_X, list_Y\n",
    "    \n",
    "    def get_dict_X(self, index):\n",
    "        file_name = self.path + self.list_X[index]\n",
    "        with open(file_name, 'rb') as pickle_file:\n",
    "            dict_X = pickle.load(pickle_file)\n",
    "        return dict_X\n",
    "    \n",
    "    def get_dict_Y(self, index):\n",
    "        filename = self.path + self.list_Y[index]\n",
    "        file = open(filename)\n",
    "        dict_Y = {}\n",
    "        for line in file:\n",
    "            data = line.split()\n",
    "            key = data[0]\n",
    "            value = ' '.join(data[1:])\n",
    "            dict_Y[key] = value\n",
    "        return dict_Y\n",
    "\n",
    "    def generate_XY(self, dict_X, dict_Y):\n",
    "        X = []\n",
    "        Y = []\n",
    "        Y_strings = []\n",
    "        input_len = []\n",
    "        label_len = []\n",
    "        \n",
    "        max_x = 0\n",
    "        max_y = 0\n",
    "        \n",
    "        for key in dict_X:\n",
    "            x_temp = dict_X[key]\n",
    "            y_temp = dict_Y[key]\n",
    "            if max_x < x_temp.shape[1]:\n",
    "                max_x = x_temp.shape[1]\n",
    "            if max_y < len(y_temp):\n",
    "                max_y = len(y_temp)\n",
    "        \n",
    "        for key in dict_X:\n",
    "            x_temp = dict_X[key]\n",
    "            y_temp = dict_Y[key]\n",
    "            Y_strings.append(y_temp)\n",
    "\n",
    "            input_len.append(x_temp.shape[1])\n",
    "            label_len.append(len(y_temp))\n",
    "            \n",
    "            to_pad_x = ( (0,0), (0, max_x - dict_X[key].shape[1]))\n",
    "            to_pad_y = (  (0, max_y - len(dict_Y[key])))\n",
    "            \n",
    "            x_temp = np.pad(dict_X[key], pad_width = to_pad_x, mode='constant', constant_values=0)\n",
    "            y_temp = self.generate_Y_array(dict_Y[key], max_y)\n",
    "            X.append(x_temp.T)\n",
    "            Y.append(y_temp)\n",
    "          \n",
    "        return np.stack(X), np.stack(Y), np.stack(input_len), np.stack(label_len), Y_strings\n",
    "\n",
    "    def generate_Y_array(self, sentence, maxlen):\n",
    "        space_token = ' '\n",
    "        end_token = '>'\n",
    "        blank_token = '%'\n",
    "        apos_token = '\\''\n",
    "        while len(sentence) != maxlen:\n",
    "            sentence += blank_token\n",
    "        sentence += end_token\n",
    "        \n",
    "        alphabet = list(ascii_uppercase) + [space_token, apos_token, blank_token, end_token] \n",
    "        char_to_index = {}\n",
    "        for idx, char in enumerate(alphabet):\n",
    "            char_to_index[char] = idx\n",
    "\n",
    "        y = []\n",
    "        \n",
    "        for char in sentence:\n",
    "            y.append(char_to_index[char])\n",
    "        \n",
    "        return np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(s1, s2):\n",
    "\n",
    "    s1 =s1.lower()\n",
    "    s2 =s2.lower()\n",
    "    b = set(s1.lower().split() + s2.lower().split())\n",
    "    \n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))/float(len(s2.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_string(indices):\n",
    "#     print(indices)\n",
    "    space_token = ' '\n",
    "    end_token = '>'\n",
    "    blank_token = '%'\n",
    "    apos_token = '\\''\n",
    "        \n",
    "    alphabet = list(ascii_uppercase) + [space_token, apos_token, blank_token, end_token] \n",
    "\n",
    "    sentence = ''\n",
    "    for idx in indices:\n",
    "        sentence += alphabet[idx]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataGenerator(train_path)\n",
    "val_data = DataGenerator(dev_path)\n",
    "test_data = DataGenerator(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_data[0]\n",
    "x, y_strings, input_len, label_len = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 727, 20)\n",
      "(58,)\n",
      "(58,)\n",
      "58\n",
      "(58, 319)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(input_len.shape)\n",
    "print(label_len.shape)\n",
    "\n",
    "print(len(y_strings))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "int64\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(x.dtype)\n",
    "print(y.dtype)\n",
    "print(input_len.dtype)\n",
    "print(label_len.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND THAT IF SHE NOTICED ANYTHING ODD OR OUT OF PLACE SHE WOULD NEVER REST UNTIL SHE HAD FERRETED OUT THE WHYS AND WHEREFORES THEREOF THERE ARE PLENTY OF PEOPLE IN AVONLEA AND OUT OF IT WHO CAN ATTEND CLOSELY TO THEIR NEIGHBOR'S BUSINESS BY DINT OF NEGLECTING THEIR OWN\n"
     ]
    }
   ],
   "source": [
    "print(y_strings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(Model):\n",
    "    def __init__(self, op_dim = 30):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.rnn = GRU(op_dim, return_sequences= True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.rnn(inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRModel(Model):\n",
    "    def __init__(self):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.base_model = BaseModel()\n",
    "        self.activation = Activation('softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.base_model(inputs)\n",
    "        x = self.activation(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASRModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, x, y_true, input_len, label_len, y_strings):\n",
    "#     print('------------------------------')\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     print(input_len.shape)\n",
    "#     print(label_len.shape)\n",
    "    \n",
    "    input_len = np.expand_dims(input_len, axis = 1)\n",
    "    label_len = np.expand_dims(label_len, axis = 1)\n",
    "#     print(input_len.shape)\n",
    "#     print(label_len.shape)\n",
    "            \n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "#         print(y_pred.shape)\n",
    "        loss = ctc_batch_cost(y_true, y_pred, input_len, label_len)\n",
    "        input_len = np.squeeze(input_len)\n",
    "        y_decode = ctc_decode(y_pred, input_len)[0][0]\n",
    "#         print(y_decode)\n",
    "#         print(len(y_strings))\n",
    "        \n",
    "        for i in range(len(y_strings)):\n",
    "            predicted_sentence = indices_to_string(y_decode[i])\n",
    "#             print(predicted_sentence)\n",
    "            accuracy = wer(y_strings[i], predicted_sentence)\n",
    "            \n",
    "        return tf.reduce_mean(loss), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_ds, epochs=5):\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        step = 0\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "        for inputs, y in train_ds:\n",
    "            x, y_strings, ip_len, label_len = inputs\n",
    "            step += 1\n",
    "            loss, accuracy = train_one_step(model, optimizer, x, y, ip_len, label_len, y_strings)\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "            \n",
    "        epoch_loss /= step\n",
    "        epoch_accuracy /= step\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        tf.print('Epoch: ', epoch, ' Loss:', epoch_loss, ' WER: ', epoch_accuracy)\n",
    "        print('Epoch: ', epoch, ' Loss:', epoch_loss, ' WER: ', epoch_accuracy)\n",
    "    \n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape = (None, None, 20))\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses, accuracies = train(model, optimizer, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
