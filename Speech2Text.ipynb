{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, Activation, Bidirectional, ConvLSTM2D, Attention, Dense, Flatten, MaxPool3D, MaxPool2D,BatchNormalization, Conv3D, GRU\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.backend import ctc_batch_cost, ctc_decode, ctc_label_dense_to_sparse, get_value\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import Levenshtein as Lev\n",
    "import sys\n",
    "from string import ascii_uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./LibriSpeech100/train/train_all/\"\n",
    "dev_path = \"./LibriSpeech100/dev/dev_all/\"\n",
    "test_path = \"./LibriSpeech100/test/test_all/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, path, to_fit = True):\n",
    "        self.path = path\n",
    "        self.list_X, self.list_Y = self.getLists()\n",
    "        self.to_fit = to_fit\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.list_X)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):      \n",
    "        dict_X = self.get_dict_X(index)   \n",
    "        dict_Y = self.get_dict_Y(index)\n",
    "        \n",
    "        X, Y, input_len, label_len, y_strings = self.generate_XY(dict_X, dict_Y)\n",
    "            \n",
    "        return [X, y_strings, input_len, label_len], Y\n",
    "    \n",
    "    \n",
    "    def getLists(self):\n",
    "        list_X = []\n",
    "        list_Y = []\n",
    "        for item in sorted(os.listdir(self.path)):\n",
    "            ext = item.split(\".\")[-1]\n",
    "            if ext == 'pkl':\n",
    "                list_X.append(item)\n",
    "            elif ext == 'txt':\n",
    "                list_Y.append(item)\n",
    "        return list_X, list_Y\n",
    "    \n",
    "    \n",
    "    def get_dict_X(self, index):\n",
    "        file_name = self.path + self.list_X[index]\n",
    "        with open(file_name, 'rb') as pickle_file:\n",
    "            dict_X = pickle.load(pickle_file)\n",
    "        return dict_X\n",
    "    \n",
    "    \n",
    "    def get_dict_Y(self, index):\n",
    "        filename = self.path + self.list_Y[index]\n",
    "        file = open(filename)\n",
    "        dict_Y = {}\n",
    "        for line in file:\n",
    "            data = line.split()\n",
    "            key = data[0]\n",
    "            value = ' '.join(data[1:])\n",
    "            dict_Y[key] = value\n",
    "        return dict_Y\n",
    "\n",
    "    \n",
    "    def generate_XY(self, dict_X, dict_Y):\n",
    "        X = []\n",
    "        Y = []\n",
    "        Y_strings = []\n",
    "        input_len = []\n",
    "        label_len = []\n",
    "        \n",
    "        max_x = 0\n",
    "        max_y = 0\n",
    "        \n",
    "        for key in dict_X:\n",
    "            x_temp = dict_X[key]\n",
    "            y_temp = dict_Y[key]\n",
    "            if max_x < x_temp.shape[1]:\n",
    "                max_x = x_temp.shape[1]\n",
    "            if max_y < len(y_temp):\n",
    "                max_y = len(y_temp)\n",
    "        \n",
    "        for key in dict_X:\n",
    "            x_temp = dict_X[key]\n",
    "            y_temp = dict_Y[key]\n",
    "            Y_strings.append(y_temp)\n",
    "\n",
    "            input_len.append(x_temp.shape[1])\n",
    "            label_len.append(len(y_temp))\n",
    "            \n",
    "            to_pad_x = ( (0,0), (0, max_x - dict_X[key].shape[1]))\n",
    "            to_pad_y = (  (0, max_y - len(dict_Y[key])))\n",
    "            \n",
    "            x_temp = np.pad(dict_X[key], pad_width = to_pad_x, mode='constant', constant_values=0)\n",
    "            y_temp = self.generate_Y_array(dict_Y[key], max_y)\n",
    "            X.append(x_temp.T)\n",
    "            Y.append(y_temp)\n",
    "          \n",
    "        return np.stack(X), np.stack(Y), np.stack(input_len), np.stack(label_len), Y_strings\n",
    "\n",
    "    \n",
    "    def generate_Y_array(self, sentence, maxlen):\n",
    "        space_token = ' '\n",
    "        end_token = '>'\n",
    "        blank_token = '%'\n",
    "        apos_token = '\\''\n",
    "        while len(sentence) != maxlen:\n",
    "            sentence += blank_token\n",
    "        sentence += end_token\n",
    "        \n",
    "        alphabet = list(ascii_uppercase) + [space_token, apos_token, blank_token, end_token] \n",
    "        char_to_index = {}\n",
    "        for idx, char in enumerate(alphabet):\n",
    "            char_to_index[char] = idx\n",
    "\n",
    "        y = []\n",
    "        \n",
    "        for char in sentence:\n",
    "            y.append(char_to_index[char])\n",
    "        \n",
    "        return np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataGenerator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataGenerator(train_path)\n",
    "val_data = DataGenerator(dev_path)\n",
    "test_data = DataGenerator(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Data loaded by the DataGenerator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_data[0]\n",
    "x, y_strings, input_len, label_len = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 727, 20)\n",
      "(58,)\n",
      "(58,)\n",
      "58\n",
      "(58, 319)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(input_len.shape)\n",
    "print(label_len.shape)\n",
    "\n",
    "print(len(y_strings))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "int64\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(x.dtype)\n",
    "print(y.dtype)\n",
    "print(input_len.dtype)\n",
    "print(label_len.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND THAT IF SHE NOTICED ANYTHING ODD OR OUT OF PLACE SHE WOULD NEVER REST UNTIL SHE HAD FERRETED OUT THE WHYS AND WHEREFORES THEREOF THERE ARE PLENTY OF PEOPLE IN AVONLEA AND OUT OF IT WHO CAN ATTEND CLOSELY TO THEIR NEIGHBOR'S BUSINESS BY DINT OF NEGLECTING THEIR OWN\n"
     ]
    }
   ],
   "source": [
    "print(y_strings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(s1, s2):\n",
    "\n",
    "    s1 =s1.lower()\n",
    "    s2 =s2.lower()\n",
    "    b = set(s1.lower().split() + s2.lower().split())\n",
    "    \n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))/float(len(s2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a tensor array to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_string(indices):\n",
    "#     print(indices)\n",
    "    space_token = ' '\n",
    "    end_token = '>'\n",
    "    blank_token = '%'\n",
    "    apos_token = '\\''\n",
    "        \n",
    "    alphabet = list(ascii_uppercase) + [space_token, apos_token, blank_token, end_token] \n",
    "\n",
    "    sentence = ''\n",
    "    for idx in indices:\n",
    "        sentence += alphabet[idx]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(Model):\n",
    "    def __init__(self, op_dim = 30):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.rnn = GRU(20, return_sequences= True)\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.time_dense = TimeDistributed(Dense(op_dim))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.rnn(inputs)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.time_dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRModel(Model):\n",
    "    def __init__(self):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.base_model = BaseModel()\n",
    "        self.activation = Activation('softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.base_model(inputs)\n",
    "        x = self.activation(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASRModel()\n",
    "model.build(input_shape = (None, None, 20))\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, x, y_true, input_len, label_len, y_strings):\n",
    "#     print('------------------------------')\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     print(input_len.shape)\n",
    "#     print(label_len.shape)\n",
    "    \n",
    "    input_len = np.expand_dims(input_len, axis = 1)\n",
    "    label_len = np.expand_dims(label_len, axis = 1)\n",
    "#     print(input_len.shape)\n",
    "#     print(label_len.shape)\n",
    "            \n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "#         print(y_pred.shape)\n",
    "        loss = ctc_batch_cost(y_true, y_pred, input_len, label_len)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    input_len = np.squeeze(input_len)\n",
    "    y_decode = ctc_decode(y_pred, input_len)[0][0]\n",
    "    \n",
    "#         print(y_decode)\n",
    "#         print(len(y_strings))\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(len(y_strings)):\n",
    "        predicted_sentence = indices_to_string(y_decode[i])\n",
    "#             print(predicted_sentence)\n",
    "        accuracy += wer(predicted_sentence, y_strings[i])\n",
    "            \n",
    "    return tf.reduce_mean(loss), accuracy/len(y_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, x, y_true, input_len, label_len, y_strings, test = False):\n",
    "    input_len = np.expand_dims(input_len, axis = 1)\n",
    "    label_len = np.expand_dims(label_len, axis = 1)\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = ctc_batch_cost(y_true, y_pred, input_len, label_len)\n",
    "    \n",
    "    input_len = np.squeeze(input_len)\n",
    "    y_decode = ctc_decode(y_pred, input_len)[0][0]\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(len(y_strings)):\n",
    "        predicted_sentence = indices_to_string(y_decode[i])\n",
    "#             print(predicted_sentence)\n",
    "        accuracy += wer(predicted_sentence, y_strings[i])\n",
    "        \n",
    "        if test:\n",
    "            print(\"Correct Sentence:\", y_strings[i])\n",
    "            print(\"Predicted Sentence:\", predicted_sentence)\n",
    "    \n",
    "    return tf.reduce_mean(loss), accuracy/len(y_strings)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, val_ds, test = False):\n",
    "    val_step = 0\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "            \n",
    "    for inputs, y in val_ds:\n",
    "        x, y_strings, ip_len, label_len = inputs\n",
    "        val_step += 1       \n",
    "        loss, accuracy = validate(model, x, y, ip_len, label_len, y_strings, test)\n",
    "        val_loss += loss\n",
    "        val_accuracy += accuracy\n",
    "                \n",
    "    val_loss /= val_step\n",
    "    val_accuracy /= val_step\n",
    "\n",
    "    tf.print(' Validation Loss:', val_loss, ' Validation WER: ', val_accuracy)\n",
    "    \n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, optimizer, train_ds, val_ds = None,epochs=20):\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        step = 0\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "        for inputs, y in train_ds:\n",
    "            x, y_strings, ip_len, label_len = inputs\n",
    "            step += 1\n",
    "            loss, accuracy = train_one_step(model, optimizer, x, y, ip_len, label_len, y_strings)\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "            if step % 78 == 0:\n",
    "                print(step)\n",
    "                \n",
    "            \n",
    "        epoch_loss /= step\n",
    "        epoch_accuracy /= step\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        tf.print('Epoch: ', epoch+1, ' Loss:', epoch_loss, ' WER: ', epoch_accuracy)\n",
    "        \n",
    "        \n",
    "        if val_ds:\n",
    "            val_loss, val_accuracy = model_evaluate(model, val_ds)\n",
    "            val_losses.append(val_loss)\n",
    "            val_acc.append(val_accuracy)\n",
    "        \n",
    "                \n",
    "    if not val_ds:    \n",
    "        return losses, accuracies\n",
    "    \n",
    "    return losses, accuracies, val_losses, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rushabh/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5783: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  1  Loss: 907.079834  WER:  0.9999876533324197\n",
      " Validation Loss: 532.587646  Validation WER:  1.000229095074456\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  2  Loss: 588.869385  WER:  1.0\n",
      " Validation Loss: 321.562958  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  3  Loss: 529.442871  WER:  1.0\n",
      " Validation Loss: 320.867584  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  4  Loss: 529.120483  WER:  1.0\n",
      " Validation Loss: 320.663605  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  5  Loss: 529.008179  WER:  1.0\n",
      " Validation Loss: 320.563293  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  6  Loss: 528.943237  WER:  1.0\n",
      " Validation Loss: 320.489929  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  7  Loss: 528.894  WER:  1.0\n",
      " Validation Loss: 320.448303  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  8  Loss: 528.858948  WER:  1.0\n",
      " Validation Loss: 320.416168  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  9  Loss: 528.819275  WER:  1.0\n",
      " Validation Loss: 320.39682  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  10  Loss: 528.795044  WER:  1.0\n",
      " Validation Loss: 320.364746  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  11  Loss: 528.751892  WER:  1.0\n",
      " Validation Loss: 320.344666  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  12  Loss: 528.667114  WER:  1.0\n",
      " Validation Loss: 320.229736  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  13  Loss: 528.175659  WER:  1.0\n",
      " Validation Loss: 319.678375  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  14  Loss: 527.450256  WER:  1.0\n",
      " Validation Loss: 319.277252  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  15  Loss: 526.081604  WER:  1.0\n",
      " Validation Loss: 318.131  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  16  Loss: 520.294922  WER:  1.0\n",
      " Validation Loss: 312.473877  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  17  Loss: 511.002045  WER:  0.9999990471562377\n",
      " Validation Loss: 309.422272  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n",
      "464\n",
      "522\n",
      "580\n",
      "Epoch:  18  Loss: 507.827576  WER:  0.9999990471562377\n",
      " Validation Loss: 308.525269  Validation WER:  1.0\n",
      "58\n",
      "116\n",
      "174\n",
      "232\n",
      "290\n",
      "348\n",
      "406\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 20, 1, 705, 133, 0]  [Op:CudnnRNNBackprop]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1e1f379dfae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-e0a15647db3f>\u001b[0m in \u001b[0;36mmodel_fit\u001b[0;34m(model, optimizer, train_ds, val_ds, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mip_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-c42c57e3bc59>\u001b[0m in \u001b[0;36mtrain_one_step\u001b[0;34m(model, optimizer, x, y_true, input_len, label_len, y_strings)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctc_batch_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/cudnn_rnn_grad.py\u001b[0m in \u001b[0;36m_cudnn_rnn_backward\u001b[0;34m(op, *grads)\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0mrnn_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rnn_mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0minput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input_mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m       direction=op.get_attr(\"direction\"))\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_cudnn_rnn_ops.py\u001b[0m in \u001b[0;36mcudnn_rnn_backprop\u001b[0;34m(input, input_h, input_c, params, output, output_h, output_c, output_backprop, output_h_backprop, output_c_backprop, reserve_space, rnn_mode, input_mode, direction, dropout, seed, seed2, name)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mrnn_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed to call ThenRnnBackward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 3, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 20, 20, 1, 705, 133, 0]  [Op:CudnnRNNBackprop]"
     ]
    }
   ],
   "source": [
    "losses, accuracies, val_losses, val_acc = model_fit(model, optimizer, train_data, val_ds = val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, acc = model_evaluate(model, test_data, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
