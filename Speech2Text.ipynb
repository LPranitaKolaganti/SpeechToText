{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, TimeDistributed, Activation, Bidirectional, ConvLSTM2D, Attention, Dense, Flatten, MaxPool3D, MaxPool2D,BatchNormalization, Conv3D, GRU\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.backend import ctc_batch_cost, ctc_decode, ctc_label_dense_to_sparse, get_value\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "import Levenshtein as Lev\n",
    "import sys\n",
    "from string import ascii_uppercase\n",
    "\n",
    "from data_generator import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"./LibriSpeech100/train/train_all/\"\n",
    "dev_path = \"./LibriSpeech100/dev/dev_all/\"\n",
    "test_path = \"./LibriSpeech100/test/test_all/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataGenerator objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataGenerator(train_path)\n",
    "val_data = DataGenerator(dev_path)\n",
    "test_data = DataGenerator(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer(s1, s2):\n",
    "\n",
    "    s1 =s1.lower()\n",
    "    s2 =s2.lower()\n",
    "    b = set(s1.lower().split() + s2.lower().split())\n",
    "    \n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    w1 = [chr(word2char[w]) for w in s1.split()]\n",
    "    w2 = [chr(word2char[w]) for w in s2.split()]\n",
    "    return Lev.distance(''.join(w1), ''.join(w2))/float(len(s2.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a tensor array to sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_to_string(indices):\n",
    "#     print(indices)\n",
    "    space_token = ' '\n",
    "    end_token = '>'\n",
    "    blank_token = '%'\n",
    "    apos_token = '\\''\n",
    "        \n",
    "    alphabet = list(ascii_uppercase) + [space_token, apos_token, blank_token, end_token] \n",
    "\n",
    "    sentence = ''\n",
    "    for idx in indices:\n",
    "        sentence += alphabet[idx]\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, op_dim = 30):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.rnn = Bidirectional(LSTM(20, return_sequences= True))\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.rnn(inputs)\n",
    "        x = self.batchnorm(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRModel(Model):\n",
    "    def __init__(self, op_dim = 30):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.rnn = Bidirectional(LSTM(20, return_sequences= True))\n",
    "        self.batchnorm = BatchNormalization()\n",
    "        self.time_dense = TimeDistributed(Dense(op_dim))\n",
    "        self.activation = Activation('softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.rnn(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.time_dense(x)\n",
    "        x = self.activation(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ASRModel()\n",
    "model.build(input_shape = (None, None, 20))\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"asr_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder (Encoder)            multiple                  6720      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection multiple                  9760      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  160       \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri multiple                  1230      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      multiple                  0         \n",
      "=================================================================\n",
      "Total params: 17,870\n",
      "Trainable params: 17,710\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Validation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, x, y_true, input_len, label_len, y_strings, test = False):\n",
    "    input_len = np.expand_dims(input_len, axis = 1)\n",
    "    label_len = np.expand_dims(label_len, axis = 1)\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = ctc_batch_cost(y_true, y_pred, input_len, label_len)\n",
    "    \n",
    "    input_len = np.squeeze(input_len)\n",
    "    y_decode = ctc_decode(y_pred, input_len)[0][0]\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(len(y_strings)):\n",
    "        predicted_sentence = indices_to_string(y_decode[i])\n",
    "#             print(predicted_sentence)\n",
    "        accuracy += wer(predicted_sentence, y_strings[i])\n",
    "        \n",
    "        if test:\n",
    "            print(\"Correct Sentence:\", y_strings[i])\n",
    "            print(\"Predicted Sentence:\", predicted_sentence)\n",
    "    \n",
    "    return tf.reduce_mean(loss), accuracy/len(y_strings)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, val_ds, test = False):\n",
    "    val_step = 0\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "            \n",
    "    for inputs, y in val_ds:\n",
    "        x, y_strings, ip_len, label_len = inputs\n",
    "        val_step += 1       \n",
    "        loss, accuracy = validate(model, x, y, ip_len, label_len, y_strings, test)\n",
    "        val_loss += loss\n",
    "        val_accuracy += accuracy\n",
    "                \n",
    "    val_loss /= val_step\n",
    "    val_accuracy /= val_step\n",
    "\n",
    "    tf.print(' Validation Loss:', val_loss, ' Validation WER: ', val_accuracy)\n",
    "    \n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, x, y_true, input_len, label_len, y_strings):\n",
    "#     print('------------------------------')\n",
    "#     print(x.shape)\n",
    "#     print(y.shape)\n",
    "#     print(input_len.shape)\n",
    "#     print(label_len.shape)\n",
    "    \n",
    "    input_len = np.expand_dims(input_len, axis = 1)\n",
    "    label_len = np.expand_dims(label_len, axis = 1)\n",
    "#     print(input_len.shape)\n",
    "#     print(label_len.shape)\n",
    "            \n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "#         print(y_pred.shape)\n",
    "        loss = ctc_batch_cost(y_true, y_pred, input_len, label_len)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    input_len = np.squeeze(input_len)\n",
    "    y_decode = ctc_decode(y_pred, input_len)[0][0]\n",
    "    \n",
    "#         print(y_decode)\n",
    "#         print(len(y_strings))\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    \n",
    "    for i in range(len(y_strings)):\n",
    "        predicted_sentence = indices_to_string(y_decode[i])\n",
    "#             print(predicted_sentence)\n",
    "        accuracy += wer(predicted_sentence, y_strings[i])\n",
    "            \n",
    "    return tf.reduce_mean(loss), accuracy/len(y_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model, optimizer, train_ds, manager, val_ds = None,epochs=20):\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        step = 0\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "        for inputs, y in train_ds:\n",
    "            x, y_strings, ip_len, label_len = inputs\n",
    "            step += 1\n",
    "            loss, accuracy = train_one_step(model, optimizer, x, y, ip_len, label_len, y_strings)\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "            if step % 78 == 0:\n",
    "                print(step)\n",
    "                \n",
    "            \n",
    "        epoch_loss /= step\n",
    "        epoch_accuracy /= step\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        tf.print('Epoch: ', epoch+1, ' Loss:', epoch_loss, ' WER: ', epoch_accuracy)\n",
    "        \n",
    "        \n",
    "        if val_ds:\n",
    "            val_loss, val_accuracy = model_evaluate(model, val_ds)\n",
    "            val_losses.append(val_loss)\n",
    "            val_acc.append(val_accuracy)\n",
    "            \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            manager.save()\n",
    "        \n",
    "                \n",
    "    if not val_ds:    \n",
    "        return losses, accuracies\n",
    "    \n",
    "    return losses, accuracies, val_losses, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = './training_checkpoints'\n",
    "ckpt = tf.train.Checkpoint(optimizer=optimizer, model = model)\n",
    "manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep = 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rushabh/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5783: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  1  Loss: 554.328613  WER:  1.0\n",
      " Validation Loss: 320.307098  Validation WER:  1.0\n",
      "78\n",
      "156\n",
      "234\n",
      "312\n",
      "390\n",
      "468\n",
      "546\n",
      "Epoch:  2  Loss: 524.982666  WER:  1.0\n",
      " Validation Loss: 313.438904  Validation WER:  1.0\n",
      "78\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies, val_losses, val_acc = model_fit(model, optimizer, train_data, manager, val_ds = val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Checkpoint and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cpkt.restore(manager.latest_checkpoint)\n",
    "\n",
    "_, acc = model_evaluate(model, test_data, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
